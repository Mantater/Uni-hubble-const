{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Error-propagation\" data-toc-modified-id=\"Error-propagation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Error propagation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analytic-error-propagation:-linear-functions\" data-toc-modified-id=\"Analytic-error-propagation:-linear-functions-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Analytic error propagation: linear functions</a></span></li><li><span><a href=\"#Analytic-error-propagation:-arbitrary-functions-of-one-variable\" data-toc-modified-id=\"Analytic-error-propagation:-arbitrary-functions-of-one-variable-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Analytic error propagation: arbitrary functions of one variable</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-Analytical-error-propagation\" data-toc-modified-id=\"Exercise:-Analytical-error-propagation-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Exercise: Analytical error propagation</a></span></li><li><span><a href=\"#Exercise:-Testing-analytical-error-propagation\" data-toc-modified-id=\"Exercise:-Testing-analytical-error-propagation-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Exercise: Testing analytical error propagation</a></span></li><li><span><a href=\"#Exercise:-Analytical-error-propagation-with-large-errors\" data-toc-modified-id=\"Exercise:-Analytical-error-propagation-with-large-errors-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Exercise: Analytical error propagation with large errors</a></span></li></ul></li><li><span><a href=\"#Analytic-error-propagation:-arbitrary-functions-of-arbitrary-numbers-of-variable\" data-toc-modified-id=\"Analytic-error-propagation:-arbitrary-functions-of-arbitrary-numbers-of-variable-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Analytic error propagation: arbitrary functions of arbitrary numbers of variable</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-2-dimensional-analytic-error-propagation\" data-toc-modified-id=\"Exercise:-2-dimensional-analytic-error-propagation-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Exercise: 2-dimensional analytic error propagation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-testing-2-dimenstional-analytic-error-propagation\" data-toc-modified-id=\"Exercise:-testing-2-dimenstional-analytic-error-propagation-1.3.1.1\"><span class=\"toc-item-num\">1.3.1.1&nbsp;&nbsp;</span>Exercise: testing 2-dimenstional analytic error propagation</a></span></li></ul></li></ul></li><li><span><a href=\"#Numerical-error-propagation:-Why-calculate-when-you-can-simulate?\" data-toc-modified-id=\"Numerical-error-propagation:-Why-calculate-when-you-can-simulate?-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Numerical error propagation: Why calculate when you can simulate?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example:-Numerical-error-propagation-when-errors-are-large\" data-toc-modified-id=\"Example:-Numerical-error-propagation-when-errors-are-large-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Example: Numerical error propagation when errors are large</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-Numerical-error-propagation-in-practice\" data-toc-modified-id=\"Exercise:-Numerical-error-propagation-in-practice-1.4.1.1\"><span class=\"toc-item-num\">1.4.1.1&nbsp;&nbsp;</span>Exercise: Numerical error propagation in practice</a></span></li><li><span><a href=\"#Adanced-Exercise:-Testing-numerical-error-propagation\" data-toc-modified-id=\"Adanced-Exercise:-Testing-numerical-error-propagation-1.4.1.2\"><span class=\"toc-item-num\">1.4.1.2&nbsp;&nbsp;</span>Adanced Exercise: Testing numerical error propagation</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Big-Picture-Exercise\" data-toc-modified-id=\"Big-Picture-Exercise-6\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Big Picture Exercise</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lesson 9:\n",
    "# working with errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Picture Exercise\n",
    "\n",
    "In astronomy, distances towards stars are often determined by measuring their *parallax* -- the angular displacement they appear to undergo (relative to distant background stars) as the Earth moves from one extreme of its orbit around the Sun to the other.\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=parallax.gif>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "<div align=\"right\">  <a href=\"https://www.gaia.ac.uk/science/parallax\"> source of image</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parallax, $p$, is usually measured in units of *milli-arcseconds* (mas), and the distance, $d$, in units of *parsecs* (pc) is then determined from this via \n",
    "\n",
    "$$d = \\frac{1000}{p}~pc,$$\n",
    "\n",
    "where, again, $p$ is measured in units of mas.\n",
    "\n",
    "* Derive an expression for the uncertainty on the distance $d$ (in pc) -- $\\sigma_d$ -- if we derive it from a measurement of the parallax $p$ with uncertainty $\\sigma_p$ (which we assume to follow a Gaussian distribution)\n",
    "\n",
    "* Use your expression to estimate the distances and associated uncertainties for stars with the following parallax measurements:\n",
    "    * $p = 10 \\pm 1$~mas\n",
    "    * $p = 27 \\pm 0.1$~mas\n",
    "    * $p = 0.5 \\pm 0.3$~mas\n",
    "    \n",
    "* Work out the uncertainties on the same three distances via numerical simulations:\n",
    "    * for each given $p$ and $\\sigma_p$, draw 10,000 Gaussian random numbers with mean $\\mu = p$ and $\\sigma = \\sigma_p$\n",
    "    * create the corresponding 10,000 distances, via $d = \\frac{1000}{p}~pc$\n",
    "    * the standard deviation of those distance is then an estimate of $\\sigma_d$\n",
    "    \n",
    "* Compare the analytical and numerical estimates for all three stars. \n",
    "    * Do all of them agree?\n",
    "    * Which of them shows the worst agreement, and why is that?\n",
    "    * For the one with the worst agreement, which estimate is better: the numerical simulation one or the analytical one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence intervals or \"what do we mean when we say $10 \\pm 1$\"?\n",
    "Suppose we have done an experiment and determined an estimate of the mass of a barbell plate, $10 \\pm 1 $kg. \n",
    "\n",
    "*What, **exactly**, do we mean by this statement?*\n",
    "\n",
    "To avoid confusion, let's agree on the following notation:\n",
    "\n",
    "* $M_{true}$ is the true mass of the barbell plate\n",
    "* $\\hat{M}$ is our *estimate* of that mass\n",
    "* $\\hat{\\sigma}_{M}$ is our *estimate* of the uncertainty on our mass estimate\n",
    "\n",
    "\n",
    "**What we mean:**\n",
    "\n",
    "* **Suppose we repeated the same experiment an infinite number of times**\n",
    "* **Each time we make an estimate of the mass, $\\hat{M}$, and also an estimate of the error, $\\hat{\\sigma}_{M}$**\n",
    "* **Then in $\\simeq$68\\% of these trials, $M_{true}$ will lie in the interval between $\\hat{M} - \\hat{\\sigma}_M$ and $\\hat{M} + \\hat{\\sigma}_M$**\n",
    "\n",
    "\n",
    "What's so special about 68\\%?\n",
    "\n",
    "* Nothing -- it's just a convention based on the shape of the Gaussian (aka \"Normal\") distribution\n",
    "\n",
    "**What we say:**\n",
    "<br><br>\n",
    "<span style=\"margin:auto; display:table; font-size:120%\">\n",
    "\"We have 68% <b>confidence</b> in the statement '$9\\,{\\rm kg} \\leq M_{true} \\leq 11\\, {\\rm kg}$'  \"\n",
    "</span>\n",
    "<br>\n",
    "<center>\n",
    "or\n",
    "</center>\n",
    "<br>\n",
    "<span style=\"margin:auto; display:table; font-size:120%\">\n",
    "\"The range from $9\\,{\\rm kg}$ to $11\\, {\\rm kg}$ is a 68% <b>confidence interval</b> for $M_{true}$.\"\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<b><u> Take-home points</b></u>\n",
    "\n",
    "* When we say things like \"$X = 10 \\pm 1$\", we are assigning probabilities to <u>*statements*</u> or <u>*claims*</u> about our experiment, based on the distribution of outcomes we'd expect to find if we repeated our experiment many times.\n",
    "<br> <br>\n",
    "* If we make the claim \"$X = Y \\pm Z$\" an infinite number of times in the same situation, $X$ will be between $Y-Z$ and $Y+Z$ 68% of those times.\n",
    "<br><br>\n",
    "* So our \"confidence\" is not so much in our specific result, but in the <u><em>process</em></u> we used to obtain this result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimating errors from measurements\n",
    "\n",
    "Suppose we want to measure some quantity $x$. Let's define some notation:\n",
    "\n",
    "* $\\hat{x}$ is our best *estimate* of this quantity, based on the results of our measurement or experiment\n",
    "\n",
    "\n",
    "* if we have multiple measurements of $x$, we'll denote their average (the sample mean) as $\\overline{x}$\n",
    "    * in fact, this will often be our best estimate of the quantity, i.e. $\\hat{x} = \\overline{x}$ in those cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In a counting experiment, if the true long-term average number of counts per 1-hr interval is $\\lambda$, then the observed number of counts, $N$, in any one 1-hr interval will follow the Poisson distribution $p(N|\\lambda)$. \n",
    "\n",
    "The question suggests using $N$ as our best estimate of $\\lambda$, i.e. \n",
    "\n",
    "$$\\lambda \\simeq \\hat{\\lambda} = N.$$\n",
    "\n",
    "Now the standard deviation of this Poisson distribution is \n",
    "\n",
    "$$\\sigma = \\sqrt{\\lambda}.$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, using the same estimate -- $\\lambda \\simeq \\hat{\\lambda} = N$ -- in the equation for the standard deviation, we get \n",
    "\n",
    "$$\\sigma \\simeq \\sqrt{N}.$$\n",
    "\n",
    "That's an estimate of the uncertainty with which we can estimate the mean count rate. So our overall estimate is \n",
    "\n",
    "$$\\hat{\\lambda} = N \\pm \\sqrt{N} = 100 \\pm 10$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If $N$ is very small -- say $N < 10$ -- use \n",
    "\n",
    "$$\\hat{\\lambda} = N \\pm \\sqrt{N + 0.75}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Estimating the error on a single measuremen from repeated measurements\n",
    "\n",
    "We measure the weight of a barbell plate $N$ times using a different scale of the same make each time. So each measurement has the the same (but unknown) intrinsic accuracy and precision. \n",
    "\n",
    "Given only these measurements, $m_i$, how do we estimate the error on each measurement, i.e. the precision of our scales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In our thought-experiment, each measurement is a random draw from the same (presumably Gaussian) distribution:\n",
    "\n",
    "* The mean of this distribution is just the true weight of the barbell plate.\n",
    "\n",
    "* Our best estimate of this true value is the mean of our measurements \n",
    "\n",
    "* The standard deviation of the distribution is set by the precision of the scales, i.e. the error in each measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since we have multiple draws from this distribution, we can *estimate* the true standard deviation just by calculating the *sample* standard deviation (which we called $s$ in Lesson 9):\n",
    "\n",
    "$$\\hat{\\sigma} = s = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N} (m_i - \\overline{m})^2}$$ \n",
    "\n",
    "Here, $\\overline{m}$ is the mean of our measurements, which we are using as an estimate of the true value, i.e.$\\hat{\\mu} \\, = \\, \\overline{m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Errors on unweighted averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's again consider our barbell plate. Let's say its true mass is $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given $N$ independent and equally precise measurements of the mass of a barbell plate:\n",
    "\n",
    "* what is our *best* estimate for the true mass, $\\hat{\\mu}$?\n",
    "\n",
    "* what is the uncertainty associated with that estimate?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Each $m_i$ is an independent, fair and equally good estimate of $m_{t}$\n",
    "\n",
    "* So our best *combined* estimate must be the average of all of these:\n",
    "\n",
    "$$\\hat{\\mu} = \\overline{m} = \\frac{1}{N} \\sum_{i=1}^{N} m_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Our estimated error on **each** measurement $m_i$ (from above) is\n",
    "\n",
    "$$\\hat{\\sigma}_{m_i} = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N} (m_i - \\overline{m})^2}$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* But the error on the **mean** of all measurements must be smaller than this. It can be shown to be \n",
    "\n",
    "$$\\sigma_{\\overline{m}} = \\frac{\\hat{\\sigma}_{m_i}}{\\sqrt{N}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**The error on the mean of $N$ measurements, each with identical error $\\sigma$, is $\\sigma/\\sqrt{N}$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Error propagation\n",
    "\n",
    "### Analytic error propagation: linear functions\n",
    "\n",
    "Suppose we have a ruler which we know to be *exactly* $L_r = 100$ cm long. Its markings aren't great though, so when we measured the length of a table with it, we obtained a result with an error, $L_t = 50 \\pm 5$ cm. \n",
    "\n",
    "What's the *combined* length of the ruler and the table, $L_{tot}$, and what is the uncertainty on this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is all pretty obvious, really. The best estimate of the combined length is clearly 150 cm. And since we know the length of the ruler *exactly*, the error on the combined length is just the error on the length of the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$L_{tot} = 150 \\pm 5\\,\\, {\\rm cm}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is $L_{tot}$ and its uncertainty expressed in *meters*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Well, this is equally obvious, hopefully. If we scale our measurement by an exact number -- as in a change in units -- then of course we also have to scale the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$L_{tot} = 1.50 \\pm 0.05\\,\\, {\\rm m}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Both of these \"operations\" probably seemed very trivial, but they get us a surprisingly long way. Because we have actually just figured out how to propagate errors across linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose we measure a quantity $x$ and obtain $\\hat{x} \\pm \\sigma_{\\hat{x}}$. However, we want to know $y = a x + b$, where $a$ and $b$ are precisely known. \n",
    "\n",
    "What's our estimate of $y$ and its uncertainty, $\\hat{y} \\pm \\sigma_{\\hat{y}}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Arguing exactly as above, we find that:\n",
    "\n",
    "$$\\hat{y} = a \\, \\hat{x} + b$$\n",
    "$$\\sigma_{\\hat{y}} = a \\, \\sigma_{\\hat{x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Actually, this is not *quite* true: after all, $a$ could be negative, but errors can't be.\n",
    "\n",
    "So the correct way of writing this is really in terms of the variances:\n",
    "\n",
    "$$\\sigma_{\\hat{y}}^2 = a^2 \\, \\sigma_{\\hat{x}}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Analytic error propagation: arbitrary functions of one variable\n",
    "\n",
    "Question: What if the functional dependence of $y$ on $x$ is **not** linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Answer: As long as our errors are *small*, we just approximate it as linear!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Suppose we have an estimate $\\hat{x}$, with a small uncertainty $\\sigma_{\\hat{x}}$ (where $\\left|\\frac{\\sigma_{\\hat{x}}}{\\hat{x}}\\right| << 1$).\n",
    "\n",
    "We want to know the corresponding uncertainty on $\\hat{y}$, given that $y = f(x)$ is some arbitrary function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Over the small interval $\\hat{x} - \\sigma_{\\hat{x}} \\lesssim x \\lesssim \\hat{x} + \\sigma_{\\hat{x}}$, we can approximate $f(x)$ as linear, with the slope being the local derivative at $x = \\hat{x}$\n",
    "<br><br>\n",
    "$$f(x) \\simeq f(\\hat{x}) + (x - \\hat{x}) \\left. \\frac{df}{dx}\\right|_{\\hat{x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<center>\n",
    "<img width=100% height=100% src=\"derivative_illustration.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "But we just saw how we can transform uncertainties for linear functions -- we just multiply the variance by the slope squared!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So if $y$ is an arbitrary function of $x$:\n",
    "<br>\n",
    "$$y = f(x),$$\n",
    "\n",
    "and we estimate $\\hat{y}$ based on a measurement of $\\hat{x}$ with error $\\sigma_{\\hat{x}}$, then the error on $\\hat{y}$ is\n",
    "<br>\n",
    "$$\\sigma_{\\hat{y}}^2 = \\left(\\left. \\frac{df}{dx}\\right|_{\\hat{x}}\\right)^2 \\, \\sigma_{\\hat{x}}^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Important:** This **only** holds for small errors, $\\left|\\sigma_{\\hat{x}}/\\hat{x}\\right| << 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For large errors, it's not OK to approximate $f(x)$ as linear over the range of uncertainties!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's test these ideas..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "    [animation 3 -- download code from Blackboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Exercise: Analytical error propagation\n",
    "\n",
    "Consider the following functions:\n",
    "\n",
    "* $y = a x^b$\n",
    "\n",
    "* $y = a \\, e^{bx}$\n",
    "\n",
    "* $y = a \\sin{x}$\n",
    "\n",
    "* $y = a \\log{b x + c}\n",
    "\n",
    "For each of these cases, assume that you've measured $x$ with an error $\\sigma_x$. \n",
    "\n",
    "Work out the corresponding error on $y$ -- $\\sigma_y$ -- assuming all other quantities ($a, b, c$) are perfectly known.\n",
    "\n",
    "Write your answers in the form \n",
    "\n",
    "$$\\frac{\\sigma_y}{y} = ...$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Exercise: Testing analytical error propagation\n",
    "\n",
    "Modify the code in s12_anim3.py and use it to test your analytical error propagation.\n",
    "\n",
    "Note that you have to modify both \"y\" and \"dydx\" in the function \"func\" (and for \"dydx\", the derivative, you'll obviously have to use the derivative you've worked out for the previous exercise).\n",
    "\n",
    "Does standard analytical error propagation always work? Are there some functions where it works well and others where it does not work so well? Can you figure out what type of function works well and for what function the approximation is not as good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Exercise: Analytical error propagation with large errors\n",
    "\n",
    "Run the simulation in the limit where the errors are pretty large, say 50% of the data, or maybe even larger than the data. Does the approximation still work? Try it again for different kind of functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Analytic error propagation: arbitrary functions of arbitrary numbers of variable\n",
    "\n",
    "What if the thing we're interested in depends on lots of different quantities:\n",
    "<br>\n",
    "$$f(x, y, z, ...)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We're not going to bother with the maths required to tackle this. Instead, we'll just note the crucial points:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**If** (and only if!) all of the variables $(x, y, z, ...)$ are **independent** then "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\sigma_{\\hat{f}}^2 \n",
    "= \\left(\\left. \\frac{\\partial f}{\\partial x}\\right|_{\\hat{x}}\\right)^2 \\, \\sigma_{\\hat{x}}^2\n",
    "+ \\left(\\left. \\frac{\\partial f}{\\partial y}\\right|_{\\hat{y}}\\right)^2 \\, \\sigma_{\\hat{y}}^2\n",
    "+ \\left(\\left. \\frac{\\partial f}{\\partial z}\\right|_{\\hat{z}}\\right)^2 \\, \\sigma_{\\hat{z}}^2\n",
    "+ ...\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Why is the error not just:\n",
    "\n",
    "$${\\bf Wrong:} \\,\\,\\,\\, \\sigma_{\\hat{f}}\n",
    "= \\left|\\left. \\frac{\\partial f}{\\partial x}\\right|_{\\hat{x}}\\right| \\, \\sigma_{\\hat{x}}\n",
    "+ \\left|\\left. \\frac{\\partial f}{\\partial y}\\right|_{\\hat{y}}\\right| \\, \\sigma_{\\hat{y}}\n",
    "+ \\left|\\left. \\frac{\\partial f}{\\partial z}\\right|_{\\hat{z}}\\right| \\, \\sigma_{\\hat{z}}\n",
    "+ ...\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Since our variables are *independent*, an error in one is just as likely to *compensate* for an error in another as it is to *excacerbate* that error. Another way of looking at it is this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Suppose the size of the errors in $f$ associated with $x$ and $y$ are the same\n",
    "\n",
    "* Our error in $x$ has a 50:50 chance of making us *overestimate* or *underestimate* $f$\n",
    "\n",
    "* Our error in $y$ has a 50:50 chance of making us *overestimate* or *underestimate* $f$\n",
    "\n",
    "* Suppose we've made an error in $x$ that made us *overestimate* f\n",
    "\n",
    "* Then our error in $y$ has a 50% chance of *doubling* that overestimate and a 50% chance of *canceling* it\n",
    "\n",
    "* So having errors in both variables does increase the possible *range* of errors in $f$ by a factor 2....\n",
    "\n",
    "* ...but it does not increase the *typical* error by this much, since half the time it actually *cancels* it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Exercise: 2-dimensional analytic error propagation\n",
    "\n",
    "Consider the following functions:\n",
    "\n",
    "* $f(x,y) = x^n + y^n$\n",
    "\n",
    "* $f(x,y) = a x^n y^m$\n",
    "\n",
    "For each of these cases, assume that you've measured $x$ and $y$ with errors $\\sigma_x$ and $\\sigma_y$, respectively. We will also assume that $x$ and $y$ are completely independent.\n",
    "\n",
    "Work out the corresponding error on $f$ -- $\\sigma_f$ -- assuming all other quantities ($a, n$) are perfectly known.\n",
    "\n",
    "Write your answers in the form \n",
    "\n",
    "$$\\left(\\frac{\\sigma_f}{f}\\right)^2 = ...$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### Exercise: testing 2-dimenstional analytic error propagation\n",
    "\n",
    "Write a program -- you may wish to adapt one of the ones above, but you don't have to -- that tests your analytical error propagation formulae. Run some test tests to confirm that the approximation is OK when the errors are small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Numerical error propagation: Why calculate when you can simulate? (absolutely important idea)\n",
    "\n",
    "What do we do if we can't -- or don't want to -- apply these methods?\n",
    "\n",
    "* Perhaps our errors in (any of) $x, y, z, ...$ are *not* small!\n",
    "\n",
    "* Perhaps we don't know the analytical form of $f(x,y,z,...)$.\n",
    "\n",
    "* Perhaps we know the form of $f(x,y,z,...)$, but the derivatives are analytically intractable.\n",
    "\n",
    "* Perhaps our variables are *not* independent.\n",
    "\n",
    "* Perhaps we don't trust ourselves not to screw up the maths.\n",
    "\n",
    "* Perhaps we're just too lazy to work out the maths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In these situations, we can do the entire error propagation numerically, by just brute-force **simulating** the things we might have seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example: Numerical error propagation when errors are large\n",
    "\n",
    "Suppose we have obtained an estimate $\\hat{x} = 3 \\pm 2$ and and $\\hat{y} = 10 \\pm 4$. I'm using the $\\hat{}$ symbol here just to remind ourselves that all we have are **estimates** of the \"true\" $x$ and $y$.\n",
    "\n",
    "Say we are interested in the quantity $f = x^2 + 3y$.\n",
    "\n",
    "It's easy to guess that our best estimate of $f$ must be $\\hat{f} = \\hat{x}^2 + 3\\hat{y} = 39$.\n",
    "\n",
    "But how can we estimate $\\sigma_{\\hat{f}}$? \n",
    "\n",
    "* We could do it analytically, by taking the derivatives of $f$ with respect to $x$ and $y$. However, in our example, the errors aren't really small compared to the actual values, so the analytical approximation won't be very good.\n",
    "\n",
    "So, how can we do it numerically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Easy: \n",
    "\n",
    "* First, we literally just create a large number of \"mock\" $x$ and $y$ measurements that we \"might have made\", based on the values and error distributions we have. \n",
    "* Then we work out the best estimate of $f$ for each pair of mock $x$ and $y$ values\n",
    "* Then the distribution of those mock $f$ values tells us the uncertainty on our value of $\\hat{f} = 39$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is some more detailed pseudo-code that hopefully explains how to do this:\n",
    "\n",
    "* we draw $N$ random numbers with mean $\\hat{x} = 3$ and standard deviation $\\sigma_{\\hat{x}} = 2$\n",
    "    * let's call these **fake_x**\n",
    "\n",
    "* we draw $N$ random numbers with mean $\\hat{y} = 10$ and standard deviation $\\sigma_{\\hat{y}} = 4$\n",
    "    * let's call these **fake_y**\n",
    "\n",
    "* we treat each pair, **fake_x** and **fake_y**, as a *mock sample*, i.e. a pair of $x,y$ values we might have observed\n",
    "    * the first  mock pair might be **fake_x** = 3.7 and **fake_y** = 6.3\n",
    "    * the second mock pair might be **fake_x** = 1.1 and **fake_y** = 15.2\n",
    "    * ...\n",
    "\n",
    "* for each pair, **fake_x** and **fake_y**, we calculate a mock value of $f$, **fake_f = fake_x^2 + 3 fake_y**\n",
    "    * the first mock pair gives **fake_f** = $3.7^2 + 3\\times6.3 = 32.59$\n",
    "    * the second mock pair gives **fake_f** = $1.1^2 + 3\\times15.2= 46.81$\n",
    "    * ...\n",
    "\n",
    "* we then estimate the error on our estimate of $\\hat{f} = 39$ as the sample standard deviation of all the **fake_f** values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### Exercise: Numerical error propagation in practice\n",
    "\n",
    "Implement the above \"pseudo-code\" and test it. \n",
    "\n",
    "##### Adanced Exercise: Testing numerical error propagation\n",
    "\n",
    "Write a wrapper around your code to test whether the numerically estimate error on $\\hat{f}$ conform to our definition of a 1$\\sigma$ confidence interval? \n",
    "\n",
    "The idea here is to run the entire procedure multiple times and test what fraction of those times the true value of $f$ lies inside within our estimated error bounds.\n",
    "\n",
    "(see the end of the notebook for a model answer, i.e. a code snippet implementing this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The nice thing about this is that it works pretty well, pretty much no matter what.\n",
    "\n",
    "However, one thing to note is that our mock samples here are **not** really a fair representation of the values we could have drawn. \n",
    "\n",
    "Why not? Because our observed values $\\hat{x}$ and $\\hat{y}$ were **not** drawn from distributions centered on $\\hat{x}$ and $\\hat{y}$. They were drawn from distributions centered on the unknown **true** values.\n",
    "\n",
    "For our present purposes, this subtle difference doesn't matter, because we're \"just\" transforming variables. All we're really trying to do is figure out how a range of $\\sigma_{\\hat{x}}$ and $\\sigma_{\\hat{y}}$ transforms into one in $\\sigma_{\\hat{f}}$.\n",
    "\n",
    "However, it's important to keep this distinction in mind. There may be situations where it *does* matter! Understanding the extent to which our mock data represents the real data is *always* worthwhile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Here is a little code snippet that implements the last exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cg/ldp5srbn6_3cc82bbxc26ppw0000gn/T/ipykernel_1010/3831814094.py:31: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  fmock[j] = f(xmock,ymock)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of times our estimate is within 1-estimated-sigma of the truth:  0.7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "nexp = 50\n",
    "nmock = 1000\n",
    "xtrue = 3\n",
    "ytrue = 10\n",
    "sigx = 2\n",
    "sigy = 4\n",
    "\n",
    "\n",
    "def f(x,y):\n",
    "    f = x*x + 3*y\n",
    "    return f\n",
    "\n",
    "ftrue = f(xtrue,ytrue)\n",
    "\n",
    "n_in_one_sigma = 0\n",
    "\n",
    "for i in np.arange(nexp):\n",
    "    #if (i%10==0):\n",
    "        #print(\"Simulating Experiment \", i)\n",
    "    xhat = np.random.normal(xtrue, sigx, size=1)\n",
    "    yhat = np.random.normal(ytrue, sigy, size=1)\n",
    "    fhat = f(xhat, yhat)\n",
    "    \n",
    "    fmock = np.zeros(nmock)\n",
    "\n",
    "    for j in np.arange(nmock):\n",
    "        xmock = np.random.normal(xhat, sigx, 1)\n",
    "        ymock = np.random.normal(yhat, sigy, 1)\n",
    "        fmock[j] = f(xmock,ymock)\n",
    "    \n",
    "    sigfhat = np.std(fmock)\n",
    "    \n",
    "    if (ftrue > (fhat-sigfhat)) and (ftrue < (fhat+sigfhat)):\n",
    "        n_in_one_sigma = n_in_one_sigma + 1\n",
    "                                    \n",
    "p = 1.0*n_in_one_sigma / nexp                                    \n",
    "print(\"Fraction of times our estimate is within 1-estimated-sigma of the truth: \", p)\n",
    "    \n",
    "         \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Picture Exercise\n",
    "\n",
    "In astronomy, distances towards stars are often determined by measuring their *parallax* -- the angular displacement they appear to undergo (relative to distant background stars) as the Earth moves from one extreme of its orbit around the Sun to the other.\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=parallax.gif>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "<div align=\"right\">  <a href=\"https://www.gaia.ac.uk/science/parallax\"> source of image</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parallax, $p$, is usually measured in units of *milli-arcseconds* (mas), and the distance, $d$, in units of *parsecs* (pc) is then determined from this via \n",
    "\n",
    "$$d = \\frac{1000}{p}~pc,$$\n",
    "\n",
    "where, again, $p$ is measured in units of mas.\n",
    "\n",
    "* Derive an expression for the uncertainty on the distance $d$ (in pc) -- $\\sigma_d$ -- if we derive it from a measurement of the parallax $p$ with uncertainty $\\sigma_p$ (which we assume to follow a Gaussian distribution)\n",
    "\n",
    "* Use your expression to estimate the distances and associated uncertainties for stars with the following parallax measurements:\n",
    "    * $p = 10 \\pm 1$~mas\n",
    "    * $p = 27 \\pm 0.1$~mas\n",
    "    * $p = 0.5 \\pm 0.3$~mas\n",
    "    \n",
    "* Work out the uncertainties on the same three distances via numerical simulations:\n",
    "    * for each given $p$ and $\\sigma_p$, draw 10,000 Gaussian random numbers with mean $\\mu = p$ and $\\sigma = \\sigma_p$\n",
    "    * create the corresponding 10,000 distances, via $d = \\frac{1000}{p}~pc$\n",
    "    * the standard deviation of those distance is then an estimate of $\\sigma_d$\n",
    "    \n",
    "* Compare the analytical and numerical estimates for all three stars. \n",
    "    * Do all of them agree?\n",
    "    * Which of them shows the worst agreement, and why is that?\n",
    "    * For the one with the worst agreement, which estimate is better: the numerical simulation one or the analytical one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
