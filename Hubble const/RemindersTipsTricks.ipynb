{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Brief-Programming-Reminders\" data-toc-modified-id=\"Brief-Programming-Reminders-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Brief Programming Reminders</a></span><ul class=\"toc-item\"><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Functions</a></span></li><li><span><a href=\"#Global-variables\" data-toc-modified-id=\"Global-variables-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Global variables</a></span></li><li><span><a href=\"#Variable-names\" data-toc-modified-id=\"Variable-names-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Variable names</a></span></li><li><span><a href=\"#Commenting-and-readability\" data-toc-modified-id=\"Commenting-and-readability-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Commenting and readability</a></span></li></ul></li><li><span><a href=\"#Data-Analysis-Reminders,-Tips-and-Tricks\" data-toc-modified-id=\"Data-Analysis-Reminders,-Tips-and-Tricks-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Analysis Reminders, Tips and Tricks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Don’t-treat-formulas-as-black-boxes!\" data-toc-modified-id=\"Don’t-treat-formulas-as-black-boxes!-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Don’t treat formulas as black-boxes!</a></span></li><li><span><a href=\"#Sanity-check-everything!\" data-toc-modified-id=\"Sanity-check-everything!-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Sanity-check everything!</a></span></li><li><span><a href=\"#Removing-correlations-between-slope-and-intercept-in-linear-fits\" data-toc-modified-id=\"Removing-correlations-between-slope-and-intercept-in-linear-fits-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Removing correlations between slope and intercept in linear fits</a></span></li><li><span><a href=\"#Combining-measurements:-mean,-weighted-mean-and/or-$\\chi^2$-fit\" data-toc-modified-id=\"Combining-measurements:-mean,-weighted-mean-and/or-$\\chi^2$-fit-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Combining measurements: mean, weighted mean and/or $\\chi^2$ fit</a></span></li><li><span><a href=\"#What's-an-acceptable-$\\chi^2$?\" data-toc-modified-id=\"What's-an-acceptable-$\\chi^2$?-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>What's an acceptable $\\chi^2$?</a></span></li><li><span><a href=\"#Errors-in-x-and-y:-finding-the-best-fit-parameters-and-their-errors\" data-toc-modified-id=\"Errors-in-x-and-y:-finding-the-best-fit-parameters-and-their-errors-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Errors in x and y: finding the best-fit parameters and their errors</a></span></li></ul></li><li><span><a href=\"#Report-Writing-Tips-and-Tricks\" data-toc-modified-id=\"Report-Writing-Tips-and-Tricks-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Report Writing Tips and Tricks</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Some Additional Reminders, Tips and Tricks\n",
    "\n",
    "All of this has been -- or will be -- covered in lectures. It's provided here again as a sort of FAQ. \n",
    "\n",
    "On the Blackboard site (under *Course Contents*) I am also providing you with a $\\chi^2$-fitting Cookbook that takes you logically through the steps involved in fitting a model to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Programming Reminders\n",
    "\n",
    "### Functions\n",
    "\n",
    "* You should use them\n",
    "\n",
    "* They should have docstrings providing a description of what the function does, and of its inputs and outputs\n",
    "\n",
    "* They should be given everything they need as explicit parameters in the function call and should return everything they calculate via a return statement\n",
    "\n",
    "\n",
    "### Global variables\n",
    "\n",
    "* Don’t use them\n",
    "\n",
    "* Seriously, don't use them\n",
    "\n",
    "\n",
    "### Variable names\n",
    "\n",
    "* Don’t use the exact same variable names in multiple places in the same program (except for obvious things like iterators, which you might name i, j or whatever)\n",
    "\n",
    "\n",
    "### Commenting and readability\n",
    "\n",
    "* Use comments \n",
    "\n",
    "* Use docstrings\n",
    "\n",
    "* Use sensible variable names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis Reminders, Tips and Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don’t treat formulas as black-boxes! \n",
    "      \n",
    "* You always have to remember why you’re doing what you’re doing, what the assumptions are etc etc\n",
    "          \n",
    "* This applies even – and especially – to error propagation formulas (such as those in your lab books)\n",
    "\n",
    "* There is a reason I didn’t give you those formulas but showed you the derivations of the “master formula” instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity-check everything!\n",
    "    \n",
    "* Do your results make sense?\n",
    "* Do the errors look realistic?\n",
    "* Does the model seem to be a decent description of the data visually?\n",
    "* Does the model have a reasonable Chi^2?\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing correlations between slope and intercept in linear fits\n",
    "\n",
    "We will be covering this in the lectures, of course, but I wanted to provide you with another take on this that might clarify this issue for some of you.\n",
    "\n",
    "\n",
    "Suppose we have a data set -- $x$, $y$ and $\\sigma_y$ -- to which we want to fit a straight line. All of the $x$ values are positive and far from zero.\n",
    "\n",
    "If we fit this data set with the model\n",
    "\n",
    "$$ y = m_1 \\, x + b_1 $$\n",
    "\n",
    "we will find that the slope, $m_1$, and the intercept, $b_1$, are strongly correlated. \n",
    "\n",
    "What does that mean? Well, if we change the slope $m_1$ a little bit away from its best-fit (lowest $\\chi^2$) value, the fit will obviously become worse and the $\\chi^2$ goes up. However, since $m_1$ and $b_1$ are correlated, we can partially compensate by also changing the intercept $b_1$.\n",
    "\n",
    "Another way of looking at this is to think about trying various slopes one by one. If slope and intercept are correlated, the intercept that will produce the best fit will be different for each trial slope.\n",
    "\n",
    "Ideally, we would prefer not to have such correlations. The reason for this is that, if slope and intercept are correlated, we cannot use standard error propagation to work out the uncertainty on any quantity that depends on both of them simultaneously.\n",
    "\n",
    "Luckily, we can remove such correlations by instead fitting the model\n",
    "\n",
    "$$ y = m_2 \\, (x - x_{0}) + b_2 $$\n",
    "\n",
    "where $x_0$ is a simple constant chosen to be close to the (weighted) average of the $x$ values.\n",
    "\n",
    "Obviously this is exactly the same line we are fitting, so we expect the best-fitting $m_1$ and $b_1$ to achieve exactly the same $\\chi^2$ as the best-fitting $m_2$ and $b_2$. In fact, a little algebra is enough to show that the slopes are in fact the same (so we can just call this $m$), but the intercepts are different:\n",
    "\n",
    "$m = m_1 = m_2$\n",
    "\n",
    "$b_1 = b_2 - m x_0$\n",
    "\n",
    "The advantage of formulating the model with $b_2$ instead of $b_1$ is just that $b_2$ is no longer correlated with $m$. We cannot trade $b_2$ off against $m$. \n",
    "\n",
    "Let's now suppose we've decided to adopt that \"shifted\" formulation. For example, let's say we've worked out the mean of the $x$ values, $\\overline{x} = 10.35753$. Based on this, we are going to adopt $x_0 = 10$, because it's a nice round value. (We're going to have to quote this value anytime we quote our model now, so we may as well make it easy to remember -- if that leaves a tiny correlation, it's not a big deal.)\n",
    "\n",
    "So, in our code, we now define a new variable to store the shifted x values, e.g.\n",
    "\n",
    "        xzero = 10\n",
    "        xshift = x - xzero\n",
    "        \n",
    "We then fit $y = m x_{shift} + b_2$ and get the best-fit $m$ and $b_2$ and their errors. We would then quote our result as \n",
    "\n",
    "$$ y = m \\, (x - 10) + b_2 $$\n",
    "\n",
    "with specific numbers and uncertainties for $m$ and $b_2$. \n",
    "\n",
    "**Now here is the point that I think has confused some people.** Suppose we now want to apply this fit, i.e.  somebody gives us a new value of $x$ -- let's call it $x_{new}$ -- and they want us to predict $y_{new}(x_{new})$, it is critical that we use **exactly the model we used**. And that model included the \"-10\". So our prediction is \n",
    "\n",
    "$$y_{new} = m \\, (x_{new} - 10) + b_2 $$\n",
    "\n",
    "And we can work out the uncertainty on this prediction via standard error propagation from the uncertainties on $m$ and $b_2$, because we've made sure they are uncorrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining measurements: mean, weighted mean and/or $\\chi^2$ fit\n",
    "\n",
    "What is the best way to combine multiple measurements (with errors) of some quantity? \n",
    "\n",
    "One answer would be the straight-up (unweighted) mean. However, this obviously doesn't make any use of the fact that we have error estimates -- it weights all the points equally, and that is (presumably) not what we want if we have errors that differ from point to point.\n",
    "\n",
    "Another answer would be the weighted mean (which we discussed very briefly in Exercise 4.4.1 in Lesson 14). This does take the errors into account, and we can also get an estimate of the uncertainty of the weighted mean.\n",
    "\n",
    "This is fine -- **so long as the things we are averaging are consistent with each other to within their errors**.\n",
    "\n",
    "Here is an example. Suppose I have two distance estimates for a galaxy -- $10 \\pm 1$ Mpc and $20 \\pm 2$ Mpc. I could obviously use my formulae for the weighted mean and its error now -- but is that a sensible thing to do? Remember, the idea here is that both $10 \\pm 1$ Mpc and $20 \\pm 2$ are random draws from a distribution centered on the true distance, with standard deviations of $1$ and $2$ Mpc, respectively. But is that plausible? Those two estimates disagree with each other by many times their uncertainties. We really wouldn't expect to see such discrepant estimates by chance.\n",
    "\n",
    "This should remind you of our discussion of $\\chi^2$ fits -- specifically what to do when our $\\chi^2$ is larger than expected for a correct model. As we saw there, we then have a choice to make -- do we still believe the model is relevant, i.e. it describes the things we care about? If so, then there must be some other source of scatter that is not included in our current error estimates -- we called that intrinsic dispersion. We can figure out how large that scatter must be and account for this and then carry on our fit as normal.\n",
    "\n",
    "As discussed in Lesson 14, for this reason it's really best to just think of the whole problem as trying to fit a single number -- i.e. a constant -- to all of the repeated measurements. The nice thing is that we can then use the standard $\\chi^2$ machinery, which includes testing for whether the fit is acceptable, and a recipe for what to do if it isn't (e.g. adding intrinsic dispersion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's an acceptable $\\chi^2$?\n",
    "\n",
    "\"My model gave a reduced $\\chi^2 = K$ -- is $K$ close enough to 1?\n",
    "\n",
    "This is covered in Lesson 13, but here is a brief reminder, along with a suggestion for how we can do a quick, qualitative check.\n",
    "\n",
    "The basic idea is this. The expectation value of the reduced $\\chi^2$ for a correct model is 1. However, every data set we model is just one particular random draw from all the equivalent data sets we might have observed. And since all of these data sets are subject to errors, each one of these will gives us a slightly different value of $\\chi^2$ when fitted by the correct model. So the expected value of $\\chi^2_{\\nu} = 1$ is just the *average* value we should expect to find -- but what matters is the distribution* of the $\\chi^2$ values for correct models around this averages. For us, here, this is just the $\\chi^2$ distribution, and the cumulative distribution function for the $\\chi^2$ distribution can be used to tell us how probable it is that we should find a value as far away from $\\chi^2_{\\nu} = 1$, if our model is actually correct.\n",
    "\n",
    "However, a qualitative short-cut that's often sufficient is to just look at the standard deviation of the $\\chi^2$ distribution. As discussed in Lesson 13, this is $\\sqrt{2\\nu}$. Here, $\\nu$ is the number of degrees of freedom, and usually $\\nu = N - M$, where $N$ is the number of data points, and $M$ is the number of model parameters. The standard deviation of the *reduced* $\\chi^2$ is therefore $\\sqrt{2/\\nu}$. We can think of this as something like a 68\\% confidence interval on the expected valueof $\\chi^2_{nu} = 1$. I.e. we would expect that $\\simeq$ 68$%$ of the time, a correct model should give a reduced $\\chi^2$ between $1 - \\sqrt{2/\\nu}$ and $1 + \\sqrt{2/\\nu}$. If we are within or close to that range, our model is OK. If we are not, either our errors are wrong or the model is not OK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors in x and y: finding the best-fit parameters and their errors\n",
    "\n",
    "This is covered in Session 16, but again, here is a reminder. If we have errors in both $x$ and $y$, we can find the best-fit parameters of a straight line ($y = mx+b$) by minimizing a slightly different version of $\\chi^2$:\n",
    "\n",
    "$$ \\chi^2 = \\sum_{i=1}^{N}  \\frac{\\left[y_i - (mx_i+b)\\right]^2}{m^2\\sigma_{x,i}^2 + \\sigma_{y,i}^2}.$$\n",
    "\n",
    "We can still derive parameter uncertainties in the same way in this case, i.e. via the $\\delta\\chi^2 = 1$ method.\n",
    "\n",
    "However, we might wonder how to do this in practice. After all, the slope $m$ now appears in the denominator!\n",
    "\n",
    "The answer is that we can't really use *curve_fit()* in that case anymore. There isn't really a good way to pass errors that are a function of $m$ into this routine.\n",
    "\n",
    "However, we absolutely *can* still use brute-force gridding. Nothing there really changes, except the expression for $\\chi^2$. So if we want to allow for errors in $x$ and $y$, brute-force gridding is the way to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Writing Tips and Tricks\n",
    "\n",
    "* Your report is effectively meant to be the \"Analysis\" section one might find in a professional research paper one might write.\n",
    "    * The sample.pdf sample.pdf document I distributed along with the assignment, should give you a good sense of what \n",
    "\n",
    "* Your report should *not* include detailed description of your code, let alone take the reader through the code. \n",
    "\n",
    "* Instead, your report should *explain* what you did and what your results were.\n",
    "\n",
    "* In deciding what to include and not to include in your report, think about reproducability:\n",
    "      \n",
    "    * you don’t have to explain anything that’s just “standard” in any detail\n",
    "          \n",
    "    * you do have to explain anything you do that’s not standard\n",
    "        * did you throw away outliers?\n",
    "        * did you have Reduced Chi^2 >> 1?\n",
    "        * did you have to “fudge” any errors?\n",
    "        * were some of your errors correlated but you treated them as independent?"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
